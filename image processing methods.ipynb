{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general file access\n",
    "src_folder = \".../IMAGE-SELECTION/full-selection/\"\n",
    "easy_folder = \".../IMAGE-SELECTION/easy10/\"\n",
    "difficult_folder = \".../IMAGE-SELECTION/difficult100/\"\n",
    "dest_folder = \".../IMAGE-SELECTION/processed/\"\n",
    "annotation_folder = \".../pascal-parts/Annotations_Part/\"\n",
    "highlight_folder=\".../IMAGE-SELECTION/highlights/\"\n",
    "types=os.listdir(src_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single image getter function\n",
    "#usable indices: starting with 0\n",
    "#easy: 0-9\n",
    "#difficult:0-99\n",
    "#all: aeroplane-385; bird-365; boat-242; bottle-116; car-370; chair-197; \n",
    "    #dog-664; horse-201; person-902; tvmonitor-134\n",
    "def get_image(category,difficulty,number):\n",
    "    if number<0:\n",
    "        print(\"only positive index allowed\")\n",
    "    else:\n",
    "        if difficulty=='easy':\n",
    "            available=os.listdir(easy_folder+category)\n",
    "            if number<len(available):\n",
    "                return Image.open(easy_folder+category+\"/\"+available[number])\n",
    "            else:\n",
    "                print(\"input number higher than number of available images\")\n",
    "        elif difficulty=='difficult':\n",
    "            available=os.listdir(difficult_folder+category)\n",
    "            if number<len(available):\n",
    "                return Image.open(difficult_folder+category+\"/\"+available[number]) \n",
    "            else:\n",
    "                print(\"input number higher than number of available images\")\n",
    "        elif difficulty=='all':\n",
    "            available=os.listdir(src_folder+category)\n",
    "            if number<len(available):\n",
    "                return Image.open(src_folder+category+\"/\"+available[number])\n",
    "            else:\n",
    "                print(\"input number higher than number of available images\")\n",
    "        else:\n",
    "            print(\"please put a valid difficulty - easy,difficult,all - in get_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get image number from simple indexing\n",
    "#usable indices: starting with 0\n",
    "#easy: 0-9\n",
    "#difficult:0-99\n",
    "#all: aeroplane-385; bird-365; boat-242; bottle-116; car-370; chair-197; \n",
    "    #dog-664; horse-201; person-902; tvmonitor-134\n",
    "def get_imagenumber(category,difficulty,number):\n",
    "    if number<0:\n",
    "        print(\"only positive index allowed\")\n",
    "    else:\n",
    "        if difficulty=='easy':\n",
    "            available=os.listdir(easy_folder+category)\n",
    "            if number<len(available):\n",
    "                return available[number].rstrip('.jpg')\n",
    "            else:\n",
    "                print(\"input number higher than number of available images\")\n",
    "        elif difficulty=='difficult':\n",
    "            available=os.listdir(difficult_folder+category)\n",
    "            if number<len(available):\n",
    "                return available[number].rstrip('.jpg')\n",
    "            else:\n",
    "                print(\"input number higher than number of available images\")\n",
    "        elif difficulty=='all':\n",
    "            available=os.listdir(src_folder+category)\n",
    "            if number<len(available):\n",
    "                return available[number].rstrip('.jpg')\n",
    "            else:\n",
    "                print(\"input number higher than number of available images\")\n",
    "        else:\n",
    "            print(\"please put a valid difficulty - easy,difficult,all - in get_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#darken single image background\n",
    "def darken_background(imagenumber,image_class,factor=0.0):\n",
    "    #load annotation and access mask data\n",
    "    mat_content = sio.loadmat(annotation_folder+imagenumber+'.mat')\n",
    "    annotation = mat_content['anno'][0,0]\n",
    "    objects = annotation[1][0]\n",
    "    #collect object annotations that match current category\n",
    "    object_pos=[]\n",
    "    for obj in objects:\n",
    "        if obj[0][0]==image_class:\n",
    "            object_pos.append(obj[2])\n",
    "    #create combined mask of >not to darken< pixels from interesting object masks\n",
    "    outline=np.zeros((333,500))\n",
    "    for pos_field in object_pos:\n",
    "        if pos_field.shape[0]>pos_field.shape[1]:\n",
    "            pos_field=np.rot90(pos_field,-1)\n",
    "        #as images are cropped to the fixed size, annotations have to be, too\n",
    "        outline+=pos_field[:333,:]\n",
    "        \n",
    "    #image access    \n",
    "    im = Image.open(src_folder+image_class+\"/\"+imagenumber+\".jpg\")\n",
    "    pixels = im.convert('RGB')\n",
    "    imarray = np.array(pixels)\n",
    "    \n",
    "    #create lightness weight matrix and apply to image channels\n",
    "    weights= np.where(outline<1,factor,1.0)\n",
    "    imarray[:,:,0]=imarray[:,:,0]*weights\n",
    "    imarray[:,:,1]=imarray[:,:,1]*weights\n",
    "    imarray[:,:,2]=imarray[:,:,2]*weights\n",
    "    #return changed image\n",
    "    return Image.fromarray(imarray)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce contrast of a single image\n",
    "def reduce_contrast(imagenumber,image_class,factor=0.4):\n",
    "    #load annotation and access mask data\n",
    "    mat_content = sio.loadmat(annotation_folder+imagenumber+'.mat')\n",
    "    annotation = mat_content['anno'][0,0]\n",
    "    objects = annotation[1][0]\n",
    "\n",
    "    #collect object annotations that match current category\n",
    "    object_pos=[]\n",
    "    for obj in objects:\n",
    "        if obj[0][0]==image_class:\n",
    "            object_pos.append(obj[2])\n",
    "\n",
    "    #create combined mask of pixels where contrast should stay normal from interesting object masks\n",
    "    outline=np.zeros((333,500))\n",
    "    for pos_field in object_pos:\n",
    "        if pos_field.shape[0]>pos_field.shape[1]:\n",
    "            pos_field=np.rot90(pos_field,-1)\n",
    "        #as images are cropped to the fixed size, annotations have to be, too\n",
    "        outline+=pos_field[:333,:]\n",
    "\n",
    "    #sub function to apply on the points\n",
    "    def contrast_reduction(pix):\n",
    "        return 128 + factor * (pix - 128)\n",
    "\n",
    "    #image access\n",
    "    im = Image.open(src_folder+image_class+\"/\"+imagenumber+\".jpg\")\n",
    "    pixels = im.convert('RGB')\n",
    "    imarray = np.array(pixels)\n",
    "    #image with reduced contrast to take pixels from where background is\n",
    "    reducedarray = np.array(im.point(contrast_reduction).convert('RGB'))\n",
    "\n",
    "    #decision matrix for 'where' function contains information where background is\n",
    "    background= np.where(outline<1,1,0)\n",
    "    imarray[:,:,0]=np.where(background,reducedarray[:,:,0],imarray[:,:,0])\n",
    "    imarray[:,:,1]=np.where(background,reducedarray[:,:,1],imarray[:,:,1])\n",
    "    imarray[:,:,2]=np.where(background,reducedarray[:,:,2],imarray[:,:,2])\n",
    "    return Image.fromarray(imarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blurring the background of single image\n",
    "def background_blurring(imagenumber,image_class,factor=4):\n",
    "    #load annotation and access mask data\n",
    "    mat_content = sio.loadmat(annotation_folder+imagenumber+'.mat')\n",
    "    annotation = mat_content['anno'][0,0]\n",
    "    objects = annotation[1][0]\n",
    "\n",
    "    #collect object annotations that match current category\n",
    "    object_pos=[]\n",
    "    for obj in objects:\n",
    "        if obj[0][0]==image_class:\n",
    "            object_pos.append(obj[2])\n",
    "    #create combined mask of pixels where no blurring should take place from interesting object masks\n",
    "    outline=np.zeros((333,500))\n",
    "    for pos_field in object_pos:\n",
    "        if pos_field.shape[0]>pos_field.shape[1]:\n",
    "            pos_field=np.rot90(pos_field,-1)\n",
    "        #as images are cropped to the fixed size, annotations have to be, too\n",
    "        outline+=pos_field[:333,:]\n",
    "\n",
    "    #image access\n",
    "    im = Image.open(src_folder+image_class+\"/\"+imagenumber+\".jpg\")\n",
    "    pixels = im.convert('RGB')\n",
    "    imarray = np.array(pixels)\n",
    "    #blurred image to take pixels from where background is\n",
    "    blurredarray = np.array((im.filter(ImageFilter.GaussianBlur(factor))).convert('RGB'))\n",
    "\n",
    "    #decision matrix for 'where' function contains information where background is\n",
    "    background= np.where(outline<1,1,0)\n",
    "    imarray[:,:,0]=np.where(background,blurredarray[:,:,0],imarray[:,:,0])\n",
    "    imarray[:,:,1]=np.where(background,blurredarray[:,:,1],imarray[:,:,1])\n",
    "    imarray[:,:,2]=np.where(background,blurredarray[:,:,2],imarray[:,:,2])\n",
    "    return Image.fromarray(imarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get image outline for single image\n",
    "def image_outline(imagenumber,image_class,factor=3):\n",
    "    #load annotation and access mask data\n",
    "    mat_content = sio.loadmat(annotation_folder+imagenumber+'.mat')\n",
    "    annotation = mat_content['anno'][0,0]\n",
    "    objects = annotation[1][0]\n",
    "\n",
    "    #collect object annotations that match current category\n",
    "    object_pos=[]\n",
    "    for obj in objects:\n",
    "        if obj[0][0]==image_class:\n",
    "            object_pos.append(obj[2])\n",
    "    #create combined outlines from interesting object masks\n",
    "    outline=np.zeros((333,500))\n",
    "    for pos_field in object_pos:\n",
    "        if pos_field.shape[0]>pos_field.shape[1]:\n",
    "            pos_field=np.rot90(pos_field,-1)\n",
    "        #as images are cropped to the fixed size, annotations have to be, too\n",
    "        location=pos_field[:333,:]\n",
    "        #check for background pixels near object pixels-> outline\n",
    "        for j in range(0,333):\n",
    "            for i in range(0,500):\n",
    "                if location[j,i]==1:\n",
    "                    if np.any(location[max(0,j-factor):min(333,j+factor+1),max(0,i-factor):min(500,i+factor+1)]==0):\n",
    "                        outline[j,i]=1\n",
    "    #outline is white, everything else black\n",
    "    return Image.fromarray(outline*255).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segment single image\n",
    "def segment_objects(imagenumber,image_class):\n",
    "    #load annotation and access mask data\n",
    "    mat_content = sio.loadmat(annotation_folder+imagenumber+'.mat')\n",
    "    annotation = mat_content['anno'][0,0]\n",
    "    objects = annotation[1][0]\n",
    "\n",
    "    #collect object annotations that match current category \n",
    "    object_pos=[]\n",
    "    for obj in objects:\n",
    "        if obj[0][0]==image_class:\n",
    "            object_pos.append(obj[2])\n",
    "\n",
    "    #image access\n",
    "    im = Image.open(src_folder+image_class+\"/\"+imagenumber+\".jpg\")\n",
    "    pixels = im.convert('RGB')\n",
    "    imarray = np.array(pixels)\n",
    "\n",
    "    #outlines for 3 color channels separately\n",
    "    outlinesa=[]\n",
    "    outlinesb=[]\n",
    "    outlinesc=[]\n",
    "    #create matrix with number of objects annotated in each pixel (to divide corresponding added colors by that number in the end)\n",
    "    overlap=np.zeros((333,500))\n",
    "    for pos_field in object_pos:\n",
    "        if pos_field.shape[0]>pos_field.shape[1]:\n",
    "            pos_field=np.rot90(pos_field,-1)\n",
    "        #as images are cropped to the fixed size, annotations have to be, too\n",
    "        location=pos_field[:333,:]\n",
    "        #when object of interest is inside current annotation matrix compute mean color of respective pixels\n",
    "        #and put those at the position of the object\n",
    "        if np.sum(location)!=0:\n",
    "            midval = np.sum(np.where(np.expand_dims(location,2),imarray,np.zeros((333,500,3))),axis=(0,1))/np.sum(location)\n",
    "            outlinesa.append(location*midval[0])\n",
    "            outlinesb.append(location*midval[1])\n",
    "            outlinesc.append(location*midval[2])\n",
    "            overlap+=location\n",
    "    \n",
    "    #map of background pixels\n",
    "    backposition=1-(np.minimum(overlap,np.ones((333,500))))\n",
    "    #number of background pixels\n",
    "    backcount=np.sum(backposition)\n",
    "    #when there are pixels that don't belong to an object (visible background)\n",
    "    if backcount!=0:\n",
    "        #compute colorchannels for the background pixels (where no interesting object was annotated)             \n",
    "        backgrounda=np.where(overlap==0,imarray[:,:,0],np.zeros((333,500)))\n",
    "        backgroundb=np.where(overlap==0,imarray[:,:,1],np.zeros((333,500)))\n",
    "        backgroundc=np.where(overlap==0,imarray[:,:,2],np.zeros((333,500)))\n",
    "\n",
    "\n",
    "        #compute mean color of background\n",
    "        backcolora=np.sum(backgrounda)/backcount\n",
    "        backcolorb=np.sum(backgroundb)/backcount\n",
    "        backcolorc=np.sum(backgroundc)/backcount   \n",
    "    else:\n",
    "        backcolora=0\n",
    "        backcolorb=0\n",
    "        backcolorc=0\n",
    "        \n",
    "    #number of objects that correspond to one pixel with their mean color\n",
    "    #replace 0 with -1 so that division by 0 is avoided, even though values on those positions are never used\n",
    "    divisor=np.where(overlap==0,np.ones((333,500))*-1,overlap)\n",
    "    #compute mean color for pixels where objects are\n",
    "    coloredobjectsa =np.where(overlap>0,sum(outlinesa)/divisor,np.zeros((333,500)))\n",
    "    coloredobjectsb =np.where(overlap>0,sum(outlinesb)/divisor,np.zeros((333,500)))\n",
    "    coloredobjectsc =np.where(overlap>0,sum(outlinesc)/divisor,np.zeros((333,500)))    \n",
    "\n",
    "    #combine background and object pixels to image\n",
    "    channela=np.where(overlap==0,np.ones((333,500))*backcolora,coloredobjectsa)\n",
    "    channelb=np.where(overlap==0,np.ones((333,500))*backcolorb,coloredobjectsb)\n",
    "    channelc=np.where(overlap==0,np.ones((333,500))*backcolorc,coloredobjectsc)\n",
    "\n",
    "    #combine color channels to image\n",
    "    imarray =np.stack((channela,channelb,channelc),axis=-1)\n",
    "\n",
    "    #operations require casting to uint8\n",
    "    return Image.fromarray(imarray.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segment single image including object parts\n",
    "def segment_parts(imagenumber,image_class):\n",
    "    #load annotation and access mask data\n",
    "    mat_content = sio.loadmat(annotation_folder+imagenumber+'.mat')\n",
    "    annotation = mat_content['anno'][0,0]\n",
    "    objects = annotation[1][0]\n",
    "    \n",
    "    #collect object annotations that match current category \n",
    "    part_pos=[]\n",
    "    for obj in objects:\n",
    "        if obj[0][0]==image_class:\n",
    "            part_pos.append(obj[3])\n",
    "    #some categories do not have part segmentation annotations and return just the object segmentation result        \n",
    "    if (part_pos[0].shape==(0,0)):\n",
    "        return segment_objects(imagenumber,image_class)\n",
    "    #image access\n",
    "    im = Image.open(src_folder+image_class+\"/\"+imagenumber+\".jpg\")\n",
    "    pixels = im.convert('RGB')\n",
    "    imarray = np.array(pixels)\n",
    "\n",
    "    #outlines for 3 color channels separately\n",
    "    outlinesa=[]\n",
    "    outlinesb=[]\n",
    "    outlinesc=[]\n",
    "    #create matrix with number of parts annotated in each pixel (to divide corresponding added colors by that number in the end)\n",
    "    overlap=np.zeros((333,500))\n",
    "    for part in part_pos:\n",
    "        #some rare image annotations strangely contain empty part annotations, those have to be ignored\n",
    "        if len(part)!=0:\n",
    "            for pos_field in part[0]:\n",
    "                part_array=pos_field[1]\n",
    "                #rotate annotation matrix if portrait shaped\n",
    "                if part_array.shape[0]>part_array.shape[1]:\n",
    "                    part_array=np.rot90(part_array,-1)\n",
    "                #as images are cropped to the fixed size, annotations have to be, too\n",
    "                location=part_array[:333,:]\n",
    "                #when a part of the object of interest is inside current annotation matrix compute mean color \n",
    "                #of respective pixels and put those at the position of the part\n",
    "                if np.sum(location)!=0:\n",
    "                    midval = np.sum(np.where(np.expand_dims(location,2),imarray,np.zeros((333,500,3))),axis=(0,1))/np.sum(location)\n",
    "                    outlinesa.append(location*midval[0])\n",
    "                    outlinesb.append(location*midval[1])\n",
    "                    outlinesc.append(location*midval[2])\n",
    "                    overlap+=location\n",
    "\n",
    "    \n",
    "    ##add object outlines for pixels that are not in any part of it (e.g. monitor frames)\n",
    "    \n",
    "    #collect object annotations that match current category \n",
    "    object_pos=[]\n",
    "    for obj in objects:\n",
    "        if obj[0][0]==image_class:\n",
    "            object_pos.append(obj[2])\n",
    "            \n",
    "    #create matrix with object pixels that are not inside any of their parts\n",
    "    outeroverlap=np.zeros((333,500))\n",
    "    for pos_field in object_pos:\n",
    "        if pos_field.shape[0]>pos_field.shape[1]:\n",
    "            pos_field=np.rot90(pos_field,-1)\n",
    "        #as images are cropped to the fixed size, annotations have to be, too\n",
    "        location=pos_field[:333,:]\n",
    "        #annotation of complete object only useful for pixels that are not in a part\n",
    "        remaining=np.maximum(np.zeros((333,500)),location-overlap)\n",
    "        #when object of interest is inside current annotation matrix compute mean color of object pixels\n",
    "        #and put those at the remaining pixels of the object\n",
    "        if np.sum(location)!=0:\n",
    "            midval = np.sum(np.where(np.expand_dims(location,2),imarray,np.zeros((333,500,3))),axis=(0,1))/np.sum(location)\n",
    "            outlinesa.append(remaining*midval[0])\n",
    "            outlinesb.append(remaining*midval[1])\n",
    "            outlinesc.append(remaining*midval[2])\n",
    "            outeroverlap+=remaining\n",
    "    \n",
    "    #total pixels that are annotated by either parts or objects (of interesting class)\n",
    "    overlap+=outeroverlap\n",
    "                \n",
    "    #map of background pixels\n",
    "    backposition=1-(np.minimum(overlap,np.ones((333,500))))\n",
    "    #number of background pixels\n",
    "    backcount=np.sum(backposition)\n",
    "    if backcount!=0:\n",
    "        #compute colorchannels for the background pixels (where no interesting object was annotated)             \n",
    "        backgrounda=np.where(overlap==0,imarray[:,:,0],np.zeros((333,500)))\n",
    "        backgroundb=np.where(overlap==0,imarray[:,:,1],np.zeros((333,500)))\n",
    "        backgroundc=np.where(overlap==0,imarray[:,:,2],np.zeros((333,500)))\n",
    "\n",
    "        #compute mean color of background\n",
    "        backcolora=np.sum(backgrounda)/backcount\n",
    "        backcolorb=np.sum(backgroundb)/backcount\n",
    "        backcolorc=np.sum(backgroundc)/backcount\n",
    "    else:\n",
    "        backcolora=0\n",
    "        backcolorb=0\n",
    "        backcolorc=0\n",
    "    \n",
    "    #number of objects that correspond to one pixels with their mean color\n",
    "    #replace 0 with -1 so that division by 0 is avoided, even though values on those positions are never used\n",
    "    divisor=np.where(overlap==0,np.ones((333,500))*-1,overlap)\n",
    "    #compute mean color for pixels where objects are\n",
    "    coloredobjectsa =np.where(overlap>0,sum(outlinesa)/divisor,np.zeros((333,500)))\n",
    "    coloredobjectsb =np.where(overlap>0,sum(outlinesb)/divisor,np.zeros((333,500)))\n",
    "    coloredobjectsc =np.where(overlap>0,sum(outlinesc)/divisor,np.zeros((333,500)))    \n",
    "\n",
    "    #combine background and object pixels to image\n",
    "    channela=np.where(overlap==0,np.ones((333,500))*backcolora,coloredobjectsa)\n",
    "    channelb=np.where(overlap==0,np.ones((333,500))*backcolorb,coloredobjectsb)\n",
    "    channelc=np.where(overlap==0,np.ones((333,500))*backcolorc,coloredobjectsc)\n",
    "\n",
    "    #combine color channels to image\n",
    "    imarray =np.stack((channela,channelb,channelc),axis=-1)\n",
    "\n",
    "    #operations require casting to uint8\n",
    "    return Image.fromarray(imarray.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segment single image with binary output: black or white\n",
    "def segment_binary(imagenumber,image_class):\n",
    "    #load annotation and access mask data\n",
    "    mat_content = sio.loadmat(annotation_folder+imagenumber+'.mat')\n",
    "    annotation = mat_content['anno'][0,0]\n",
    "    objects = annotation[1][0]\n",
    "\n",
    "    #collect object annotations that match current category \n",
    "    object_pos=[]\n",
    "    for obj in objects:\n",
    "        if obj[0][0]==image_class:\n",
    "            object_pos.append(obj[2])\n",
    "\n",
    "    #create positiv-zero map for object locations\n",
    "    overlap=np.zeros((333,500))\n",
    "    for pos_field in object_pos:\n",
    "        if pos_field.shape[0]>pos_field.shape[1]:\n",
    "            pos_field=np.rot90(pos_field,-1)\n",
    "        location=pos_field[:333,:]\n",
    "        #when object of interest is inside current annotation matrix add points to positive area\n",
    "        if np.sum(location)!=0:\n",
    "            overlap+=location\n",
    "\n",
    "    shape=np.where(overlap==0,0,255)\n",
    "    #create image from positive-zero map\n",
    "    imarray =np.stack((shape,shape,shape),axis=-1)\n",
    "    #type has to be specified for Image casting to work\n",
    "    return Image.fromarray(imarray.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put out every version of 2 easy images from every category in the highlights folder as examples\n",
    "for typee in types:\n",
    "    print(typee)\n",
    "    for i in range(6,8):\n",
    "        get_image(typee,'easy',i).save(highlight_folder+typee+str(i%2+1)+\"original.jpg\",quality=100)\n",
    "        background_blurring(get_imagenumber(typee,'easy',i),typee).save(highlight_folder+typee+str(i%2+1)+\"blurred.jpg\",quality=100)\n",
    "        darken_background(get_imagenumber(typee,'easy',i),typee).save(highlight_folder+typee+str(i%2+1)+\"darkened.jpg\",quality=100)\n",
    "        reduce_contrast(get_imagenumber(typee,'easy',i),typee).save(highlight_folder+typee+str(i%2+1)+\"contrast.jpg\",quality=100)\n",
    "        image_outline(get_imagenumber(typee,'easy',i),typee).save(highlight_folder+typee+str(i%2+1)+\"outline.jpg\",quality=100)\n",
    "        segment_binary(get_imagenumber(typee,'easy',i),typee).save(highlight_folder+typee+str(i%2+1)+\"binary.jpg\",quality=100)\n",
    "        segment_objects(get_imagenumber(typee,'easy',i),typee).save(highlight_folder+typee+str(i%2+1)+\"seg-objects.jpg\",quality=100)\n",
    "        segment_parts(get_imagenumber(typee,'easy',i),typee).save(highlight_folder+typee+str(i%2+1)+\"seg-parts.jpg\",quality=100)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply all image modifications on all images in a category\n",
    "def process_all(typee):\n",
    "    current_folder_items = os.listdir(src_folder+typee+\"/\")\n",
    "    for image in current_folder_items:\n",
    "        imagenumber=image.rstrip('.jpg')\n",
    "        background_blurring(imagenumber,typee).save(dest_folder+\"background-blurring/\"+typee+\"/\"+imagenumber+\".jpg\",quality=100)\n",
    "        darken_background(imagenumber,typee).save(dest_folder+\"background-dark/\"+typee+\"/\"+imagenumber+\".jpg\",quality=100)\n",
    "        reduce_contrast(imagenumber,typee).save(dest_folder+\"background-contrast/\"+typee+\"/\"+imagenumber+\".jpg\",quality=100)\n",
    "        image_outline(imagenumber,typee).save(dest_folder+\"outline-only/\"+typee+\"/\"+imagenumber+\".jpg\",quality=100)\n",
    "        segment_binary(imagenumber,typee).save(dest_folder+\"segment-binary/\"+typee+\"/\"+imagenumber+\".jpg\",quality=100)\n",
    "        segment_objects(imagenumber,typee).save(dest_folder+\"segment-object/\"+typee+\"/\"+imagenumber+\".jpg\",quality=100)\n",
    "        segment_parts(imagenumber,typee).save(dest_folder+\"segment-parts/\"+typee+\"/\"+imagenumber+\".jpg\",quality=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 1.46 s, total: 1min 24s\n",
      "Wall time: 1min 24s\n",
      "CPU times: user 52.1 s, sys: 2.27 s, total: 54.4 s\n",
      "Wall time: 54.7 s\n",
      "CPU times: user 55.8 s, sys: 1.77 s, total: 57.5 s\n",
      "Wall time: 58 s\n",
      "CPU times: user 1min 33s, sys: 1.76 s, total: 1min 35s\n",
      "Wall time: 1min 36s\n",
      "CPU times: user 1min 7s, sys: 635 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n",
      "CPU times: user 1min 2s, sys: 987 ms, total: 1min 3s\n",
      "Wall time: 1min 3s\n",
      "CPU times: user 1min 22s, sys: 3.53 s, total: 1min 25s\n",
      "Wall time: 1min 26s\n",
      "CPU times: user 44.2 s, sys: 1.49 s, total: 45.7 s\n",
      "Wall time: 46 s\n",
      "CPU times: user 54 s, sys: 2.52 s, total: 56.6 s\n",
      "Wall time: 56.8 s\n",
      "CPU times: user 1min 16s, sys: 2.21 s, total: 1min 19s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "for typee in types:\n",
    "    %time process_all(typee)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
